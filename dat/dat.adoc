= Dossier d'Architecture Technique

:toc:

Ce document décrit l'architecture technique du système OPUS.

Le système OPUS implémente des processus SOFTEAM.

* Gérer les utilisateurs, les équipes, les compétences, les plans de charges, ...

== ARCHITECTURE LOGIQUE

image::assets/ArchitectureLogique.png[]

=== Description

L'architecture d'OPUS est construit comme un ensemble d'applications autonomes.

La conception du système est basée sur le pattern https://martinfowler.com/articles/201701-event-driven.html[Event-Carried State Transfer]. C'est le mode de communication principal entre les applications.

* Les événements métier sont publiés sur le bus de message. Ceci permet par exemple a une application de s'abonner aux événements de création d'utilisateur pour conserver sa base de données d'utilisateurs à jour.

De cette manière, chaque application dispose d'une base de données locale qui lui permet de stocker ses données propres ainsi qu'une vue des données (des autres applications) nécessaires à son fonctionnement. Chaque application gère par exemple sa propre liste d'utilisateurs en ne filtrant que les informations utiles.

L'architecture implémente aussi des patterns https://martinfowler.com/articles/enterpriseREST.html[REST].

* Si un service A d'une application a besoin d'une donnée gérée par un service B d'une autre application (pour réaliser un traitement), le service A appelle le service B en mode synchrone.

Ces mécanismes de communication asynchrones simplifient l'intégration de nouvelles applications.

=== Composants

.Composants
|===
|Composant| Fonction

|Keycloak
|Serveur d'Authentification

|Kafka
|Bus de messages

|Blob Management
|Gestion du stockage de Fichiers

|User Management
|Gestion des données Utilisateur

|Mission Management
|Gestion des données Mission

|Team Management
|Gestion des données Equipe

|User Skill Management
|Gestion des compétences des Utilisateurs

|Team Skill Management
|Gestion des compétences des Equipes

|Plan de charge
|Plan de charge

|Carnet de commandes
|Carnet de commandes

|Calendrier
|Calendrier des Utilisateurs et Gestion d'Alertes

|Tableau des effectifs
|Gestion des effectifs

|Salesforce
|Salesforce

|CV Gen
|Application SOFTEAM de Gestion de CV

|===

== ARCHITECTURE PHYSIQUE

L'architecture est basée sur deux points :

* L'utilisation de Kubernetes pour la gestion d'applications conteneurisées
* L'utilisation de partitions NFS pour la persistance de données

=== Cluster Kubernetes

Le cluster Kubernetes est un orchestrateur Docker : il permet d'automatiser le déploiement, la mise à l'échelle (scaling), la gestion d'applications conteneurisées.

K8s ne gère pas la persistance des données des applications. K8s permet néanmoins aux conteneurs d'utiliser des moyens de persistance externes (exemple: partition NFS).

Le cluster est composé de plusieurs nœuds : chaque nœud est une VM. Il existe deux types de nœuds : des nœuds maître (master) et des nœuds esclave (slave).

=== Partitions NFS

Les partitions NFS sont fournies par OVH.

=== Dimensionnement

Pour dimensionner l'architecture physique, nous nous basons sur l'hypothèse que les besoins principaux des applications sont des besoins de RAM. En effet, les applications déployées sont des applications java (besoin de RAM important) peu sollicitées (besoin de CPU faible).

Ensuite, nous estimons les besoins globaux (cf. <<dimensionnement_apps>>) en RAM des applications.

En partant de cette estimation globale, nous calculons pour différents types de VMs le nombre de VM nécessaires (cf. <<dimensionnement_serveurs>>).

Pour calculer le nombre de nœuds il faut prendre en compte l'exécution, sur chaque nœud :

* d'un agent https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/[kubelet]
* d'un agent https://www.elastic.co/fr/products/beats/filebeat[FileBeat] pour la centralisation des logs

#VM = #RAM Total / (#RAM VM - #RAM Kubelet - # RAM FileBeat)

[[dimensionnement_serveurs]]
.Dimensionnement des Serveurs
|===
|RAM / Instance| #Instance

|8 Go
|6,615384615

|64 Go
|0,688
|===

[[dimensionnement_apps]]
.Dimensionnement des Applications
|===
|Service| Techno| RAM / Instance| Disk / Instance| Instance| Total RAM| Total Disk

|Jenkins
|Java
|4 Go
|20 Go
|1
|4 Go
|20 Go

|Nexus
|Java
|4 Go
|30 Go
|1
|4 Go
|30 Go

|SonarQube
|Java
|4 Go
|30 Go
|1
|4 Go
|30 Go

|Keycloak
|Java
|1 Go
|2 Go
|1
|1 Go
|2 Go

|Kibana
|Node
|1 Go
|1 Go
|1
|1 Go
|1 Go

|ElasticSearch
|Java
|2 Go
|5 Go
|2
|4 Go
|10 Go

|Blob Mgmt
|C#
|1 Go
|1 Go
|2
|2 Go
|2 Go

|Blob Mgmt Database
|MongoDB
|512 Mo
|1 Go
|2
|1 Go
|2 Go

|User Mgmt
|C#
|1 Go
|1 Go
|2
|2 Go
|2 Go

|User Mgmt Database
|MongoDB
|512 Mo
|1 Go
|2
|1 Go
|2 Go

|Mission Mgmt
|Kotlin
|1 Go
|1 Go
|2
|2 Go
|2 Go

|Mission Mgmt Database
|PostgreSQL
|512 Mo
|1 Go
|2
|1 Go
|2 Go

|Team Mgmt
|Node
|1 Go
|1 Go
|2
|2 Go
|2 Go

|Team Mgmt Database
|PostgreSQL
|512 Mo
|1 Go
|2
|1 Go
|2 Go

|Team Skill Mgmt
|Kotlin
|1 Go
|1 Go
|2
|2 Go
|2 Go

|Team Skill Mgmt Database
|Kafka
|2 Go
|10 Go
|3
|6 Go
|30 Go

|User Skill Mgmt
|Java
|1 Go
|1 Go
|2
|2 Go
|2 Go

|User Skill Mgmt Database
|ElasticSearch
|0
|0
|0
|0
|0

|Total
|
|
|
|32
|43 Go
|195 Go

|===

.Dimensionnement des nœuds du cluster
|===
|Service| RAM

|Kubernetes Node
|1 Go

|FileBeat
|0,5 Go
|===

=== Pricing

.Pricing
|===
|Type Serveur| RAM / Instance| vCPU / Instance|  Prix / Instance| #Instance|  RAM Total| CPU Total| Prix Total

|VPS SSD 3
|8 Go
|2 vCPU
|12,99 €
|7
|64 Go
|16 vCPU
|77,94 €

|https://www.ovh.com/fr/serveurs_dedies/enterprise/1801sp01.xml[SP-64]
|64 Go
|4c/8t
|99,99 €
|1
|64 Go
|4c/8t
|99,99 €

|https://www.ovh.com/fr/serveurs_dedies/enterprise/1801sp94.xml[SP-128-S]
|128 Go
|8c/16t
|169,99 €
|1
|128 Go
|8c/16t
|169,99 €

|===

== SÉCURITÉ

=== Connexion SSH

La connexion SSH aux VMs du cluster est réalisé par enregistrement de clefs SSH.

=== Authentification applicatif

La gestion de l'authentification/autorisation est géré par mise en place du protocole https://openid.net/connect/[OpenID Connect].

Le Flow a utiliser est "Authorization Code Flow" : ce processus permet à un utilisateur de s'authentifier via un navigateur Web, à une application Web qui a un BackEnd capable de gérer des secrets (c'est le cas de nos applications).

Quand un service appelle un autre service, il passe le token d'authentification dans les entêtes de la requête.

== RÉSILIENCE

=== Tolérance aux Pannes

Nous gérons deux types de pannes : les pannes des applications et les pannes du cluster.

==== Pannes des applications

La gestion des pannes des applications est gérée par Kubernetes.

Pour y arriver, Kubernetes se base sur https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/[les lignes de vie des applications].

Si la ligne de vie d'une application ne répond pas, Kubernetes se charge de redémarrer l'application. Chaque application déployée doit donc définir ses lignes de vie.

....
@TAG SCN_APP_HEALTHCHECK
Scenario: Application HealthCheck
Given I am a developer of an application
When the applications probes do not respond
Then the Kubernetes restarts the application
....

==== Pannes du Cluster

La gestion des pannes du cluster est gérée de deux manières.

La première solution consiste à faire un backup des données du cluster. En cas de panne du master, nous pouvons recréer un master identique au master en panne (en repartant des données du backup).

L'état du cluster est géré via une base de données clef-valeur https://coreos.com/etcd/[etcd].

La seconde solution plus complexe consiste à réaliser une installation multi-maîtres. Dans ce cas, la brique qui contient l'état du cluster (i.e. le serveur etcd) est redondé.

Remarque :

* Pour être tolérant à une panne, il faut 3 maîtres
* Pour être tolérant à deux pannes, il faut 5 maîtres

== EXIGENCES

=== REQ_K8S_BACKUP

The Kubernetes state must be backed up regularly; The Kubernetes state must be restorable from backup Snapshots.