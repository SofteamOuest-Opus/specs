= Dossier d'Architecture Technique

:toc:

== ARCHITECTURE LOGIQUE

image::assets/ArchitectureLogique.png[]

=== Description

L'objectif du système OPUS est de modéliser les processus de SOFTEAM.

OPUS est construit comme un ensemble d'applications autonomes.

Chaque application dispose d'une base de données locale qui lui permet de stocker ses données propre ainsi qu'une vue des données (des autres applications) nécessaires à son fonctionnement. Chaque application gère par exemple sa propre liste d'utilisateurs en ne conservant que les informations utiles.

Les applications communiquent de manière synchrone et asynchrone.

* Si un service A d'une application a besoin d'une donnée gérée par un service B d'une autre application (pour réaliser un traitement), le service A appelle le service B en mode synchrone.

* Les événements métier sont publiés sur le bus de message. Ceci permet par exemple a une application de s'abonner aux événements de création d'utilisateur pour conserver sa base de données d'utilisateurs à jour.

=== Composants

.Composants
|===
|Composant| Fonction

|Keycloak
|Serveur d'Authentification

|Kafka
|Bus de messages

|Blob Management
|Gestion du stockage de Fichiers

|User Management
|Gestion des données Utilisateur

|Mission Management
|Gestion des données Mission

|Team Management
|Gestion des données Equipe

|User Skill Management
|Gestion des compétences des Utilisateurs

|Team Skill Management
|Gestion des compétences des Equipes

|Plan de charge
|Plan de charge

|Carnet de commandes
|Carnet de commandes

|Calendrier
|Calendrier des Utilisateurs et Gestion d'Alertes

|Tableau des effectifs
|Gestion des effectifs

|Salesforce
|Salesforce

|CV Gen
|Application SOFTEAM de Gestion de CV

|===

== ARCHITECTURE PHYSIQUE

=== Cluster Kubernetes

Le cluster Kubernetes est un orchestrateur Docker : il permet d'automatiser le deploiement, le scaling, la gestion d'applications conteneurisées.

K8s ne gère pas la persistance des données des applications. K8s permet néanmoins aux conteneurs d'utiliser des moyens de persistance externes (exemple: partition NFS).

Le cluster est composé de plusieurs noeuds : chaque noeud est une VM. Il existe deux types de noeuds : des noeuds maître (master) et des noeuds esclave (slave).

L'état du cluster est géré via une base de données clef-valeur https://coreos.com/etcd/[etcd].



=== Partitions NFS

== SECURITE

=== Authentification

La gestion de l'authentification/autorisation est géré par mise en place du protocole https://openid.net/connect/[OpenID Connect].

Le Flow a utiliser est "Authorization Code Flow" : ce processus permet à un utilisateur de s'authentifier via un navigateur Web, à une application Web qui a un BackEnd capable de gérer des secrets (c'est le cas de nos applications).

Quand un service appelle un autre service, il passe le token d'authentification dans les entêtes de la requête.

== RESILIENCE

=== Tolérance aux Pannes

Nous gérons deux types de pannes : les pannes des applications et les pannes du cluster.

==== Pannes des applications

La gestion des pannes des applications est gérée par Kubernetes.

Pour y arriver, Kubernetes se base sur https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/[les lignes de vie des applications].

Si la ligne de vie d'une application ne répond pas, Kubernetes se charge de redémarrer l'application. Chaque application déployée doit donc définir ses lignes de vie.

....
@TAG SCN_APP_HEALTHCHECK
Scenario: Application HealthCheck
Given I am a developer of an application
When the applications probes do not respond
Then the Kubernetes restarts the application
....

==== Pannes du Cluster

La gestion des pannes du cluster est gérée de deux manières.

La première solution consiste à faire un backup des données du cluster. En cas de panne du master, nous pouvons recréer un master identique au master en panne (en repartant des données du backup).

La seconde solution plus complexe consiste à réaliser une installation multi-maîtres. Dans ce cas, la brique qui contient l'état du cluster (le serveur etcd) est redondé.

Remarque : 
* Pour être tolérant à une panne, il faut 3 maîtres
* Pour être tolérant à deux pannes, il faut 5 maîtres

=== Haute Disponibilité

=== Monitoring

.Dimensionnement
|===
|Service| Techno| RAM / Instance| Disk / Instance| Instance| Total RAM| Total Disk

|Jenkins
|Java
|4 Go
|20 Go
|1
|4 Go
|20 Go

|Nexus
|Java
|4 Go
|30 Go
|1
|4 Go
|30 Go

|SonarQube
|Java
|4 Go
|30 Go
|1
|4 Go
|30 Go

|Keycloak
|Java
|1 Go
|2 Go
|1
|1 Go
|2 Go

|Kibana
|Node
|1 Go
|1 Go
|1
|1 Go
|1 Go

|ElasticSearch
|Java
|2 Go
|5 Go
|2
|4 Go
|10 Go

|Blob Mgmt
|C#
|1 Go
|1 Go
|2
|2 Go
|2 Go

|Blob Mgmt Database
|MongoDB
|512 Mo
|1 Go
|2
|1 Go
|2 Go

|User Mgmt
|C#
|1 Go
|1 Go
|2
|2 Go
|2 Go

|User Mgmt Database
|MongoDB
|512 Mo
|1 Go
|2
|1 Go
|2 Go

|Mission Mgmt
|Kotlin
|1 Go
|1 Go
|2
|2 Go
|2 Go

|Mission Mgmt Database
|PostgreSQL
|512 Mo
|1 Go
|2
|1 Go
|2 Go

|Team Mgmt
|Node
|1 Go
|1 Go
|2
|2 Go
|2 Go

|Team Mgmt Database
|PostgreSQL
|512 Mo
|1 Go
|2
|1 Go
|2 Go

|Team Skill Mgmt
|Kotlin
|1 Go
|1 Go
|2
|2 Go
|2 Go

|Team Skill Mgmt Database
|Kafka
|2 Go
|10 Go
|3
|6 Go
|30 Go

|User Skill Mgmt
|Java
|1 Go
|1 Go
|2
|2 Go
|2 Go

|User Skill Mgmt Database
|ElasticSearch
|0
|0
|0
|0
|0

|Total
|
|
|
|32
|43 Go
|195 Go

|===

.Dimensionnement de chaque Noeud
|===
|Service| RAM

|Kubernetes Node
|1 Go


|FileBeat
|0,5 Go
|===

.Nombre d'instances
|===
|RAM / Instance| #Instance

|8 Go
|5,1875

|64
|0,6484375
|===

.Pricing
|===
|Type Serveur| RAM / Instance| vCPU / Instance|  Prix / Instance| #Instance|  RAM Total| CPU Total| Prix Total

|VPS SSD 3
|8 Go
|2 vCPU
|12,99 €
|6
|64 Go
|16 vCPU
|77,94 €

|SP-64
|64 Go
|4c/8t
|99,99 €
|1
|64 Go
|4c/8t
|99,99 €

|SP-128-S
|128 Go
|8c/16t
|169,99 €
|1
|128 Go
|8c/16t
|169,99 €

|===

== Exigences

=== REQ_K8S_BACKUP

The Kubernetes state must be backed up regularly; The Kubernetes state must be restored from backup Snapshots.