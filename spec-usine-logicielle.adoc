= Usine Logicielle

Nous mettons en place l'Usine Logicielle pour industrialiser nos développements et notamment accélérer les temps de mises en prod en suivant une démarche de Continuous Delivery : tout commit sur un dépôt GIT d'une application déclenche la construction des binaires de l'application suivi du déploiement de l'application.

Pour ce faire, l'Usine Logicielle fournit des outils pour automatiser la construction des binaires ainsi que des environnements de déploiement.

== Outillage

L'Outillage mis en place permet :

* La gestion des sources des applications
* La construction des binaires de l'application et l'exécution des tests unitaires (via Jenkins)
* L'archivage des binaires (dans Nexus)
* L'analyse qualité des sources (via SonarQube)

=== Gestion des Sources

Les sources sont hébergées dans une Organisation Github https://github.com/SofteamOuest-Opus[dédiée].

Les sources d'une application sont gérées dans un dépôt GIT.

=== Jenkins

Nous utilisons https://jenkins.k8.wildwidewest.xyz[Jenkins] pour orchestrer les BUILDS.

==== Configuration Jenkins

De manière à simplifier au maximum la mise en place de l'Usine Logicielle, le serveur Jenkins est configuré automatiquement, en utilisant https://jenkins.io/projects/jcasc/[Jenkins Configuration as Code].

[source]
....
Scenario: JENKINS_CONF
En tant que Gestionnaire Intégration Continue,
Quand j'importe un fichier de configuration du serveur Jenkins
Alors le serveur est correctement configuré
....

La configuration concerne notamment :

* Les comptes utilisateurs
* Les secrets (mot de passe Nexus, token sonarqube)
* Les fichiers de configuration (Gradle, SSH)
* L'import d'organisations GitHub
* L'import de plugins (exemple : JIRA)

==== Jobs Jenkins

Nous utilisons Jenkins pour orchestrer nos BUILDS. De manière à uniformiser nos jobs Jenkins, nous les implémentons via des Jenkinsfile.

==== Job de BUILD

Le Job de BUILD d'une application permet de construire les binaires (image Docker) de l'application (en version SNAPSHOT) et déclencher un Job de Déploiement

[source]
....
Scenario: JENKINS_JOB_BUILD
En tant qu'Utilisateur IC,
Quand j'exécute le Job de BUILD d'une application
Alors le job construit et déploie l'image Docker de l'application
Et déclenche le Job du RUN sur l'application
....

RG : Le Job de BUILD est déclenché automatiquement après tout push sur les sources de l'application.

==== Job de RELEASE

Le Job de RELEASE permet de créer une RELEASE des sources et binaires d'une application.

[source]
....
Scenario: JENKINS_JOB_RELEASE
En tant que Responsable Livraison d'une application,
Quand j'exécute le Job de RELEASE de cette application en indiquant la version de RELEASE
Alors le Job construit l'image Docker de l'application en version RELEASE et le déploie sur Nexus
Et construit le jars de l'application en version RELEASE et le déploie sur Nexus
....

==== Job de RUN

Le Job de RUN permet de déployer une application sur le cluster.

[source]
....
Scenario: JENKINS_JOB_RUN
En tant qu'Utilisateur IC,
Quand j'exécute le Job de RUN en indiquant
* le nom de l'application
* la version de l'application
* la version du package Helm
* l'environnement de déploiement (dev, prod)
Alors l'application est déployée
....

==== Job de FULL-RUN

Le Job de FULL-RUN permet de déployer toutes les applications.

Le Job prend les même paramètres que le Job de RUN (pour chaque application à déployer).

[source]
....
Scenario: JENKINS_JOB_FULL_RUN
En tant qu'Utilisateur IC,
Quand j'exécute le Job de FULL-RUN en indiquant pour chaque application
* le nom de l'application
* la version de l'application
* la version du package Helm
* l'environnement de déploiement (dev, prod)
Alors toutes les applications sont déployées
....

RG : Si le tag de l'image Docker d'une application n'est pas défini, l'application n'est pas déployée.

=== Nexus

Nous utilisons https://Nexus.k8.wildwidewest.xyz/[Nexus] pour archiver les jars RELEASE ainsi que les images Docker des applications.

RG : Les accès au serveur Nexus par les Jobs de BUILD sont authentifiés par login, password.

=== SonarQube

Nous utilisons https://sonarqube.k8.wildwidewest.xyz/[SonarQube] pour analyser la qualité du code développé.

RG : Les accès au serveur SonarQube par les Jobs de BUILD sont authentifiés par token.

=== JIRA

Nous utilisons https://wildwidewest.atlassian.net[JIRA] pour suivre le développement du Projet.

== Environnements

Nous déployons nos applications dans différents environnements.

* L'environnement de DEV permet de tester la dernière version SNAPSHOT des applications (=> version en cours de développement).
* L'environnement de RE7 permet de tester une version RELEASE avant Mise en Prod (=> version à qualifier).
* L'environnement de PROD correspond à l'environnement de PROD (=> version mise à dispo des utilisateurs).

De manière à isoler les applications des différents environnements,

* Chaque environnement est matérialisé par un namespace dans le cluster Kubernetes
* Les namespaces sont isolés

[source]
....
Scenario: KUBERNETES_NAMESPACE
En tant qu'Utilisateur IC
Quand je déploie une application dans un environnement donné,
Alors l'application est déployée dans un namespace Kubernetes de nom similaire
....

[source]
....
Scenario: KUBERNETES_NAMESPACE_ISOLATION
En tant que Gestion Kubernetes,
Quand je déploie 2 namespaces,
Alors les deux namespaces sont isolés (un POD d'un namespace ne doit pouvoir appeler les services d'un POD d'un autre namespace)
....

=== Accès aux Applications

Le nom de l'environnement apparaît dans l'URL de l'application.

Si le nom de l'application est mon application,

* Si le nom de l'environnement est PROD, l'URL d'accès est https://monapplication.k8.wildwidewest.xyz
* Sinon (le nom de l'environnement est XXX), l'URL d'accès est https://monapplication-XXX.k8.wildwidewest.xyz (=> le nom de l'environnement se retrouve dans l'URL de l'application)

=== Centralisation des Logs

La centralisation des Logs permet d'analyser via une IHM unique les logs de toutes les applications.

Pour simplifier la mise en place, les applications partagent un même format de Logs.

Le format du Log est :

* Niveau de Log : DEBUG, INFO, ERROR
* Message du Log
* Nom de l'application
* Nom de l'environnement
* Id de Correlation du Message
* StackTrace si le message est une exception

[source]
....
Scenario:
En tant que Développeur d'une application,
Quand mon application log un message,
Alors ce message est remonté dans le serveur de Centralisation des Logs
....

RG : Les accès au serveur de Centralisation des logs sont authentifiés par token.

=== Monitoring

Le monitoring permet de monitorer l'état du cluster Kubernetes.

Nous monitorons :

* Le CPU (critère à définir)
* Le disque (utilisation > 90 %)

Les alertes de monitoring sont envoyées par email aux membres du projet.

RG : Les accès au serveur de Centralisation des logs sont authentifiés par token.

=== Tolérance aux Pannes

Nous gérons deux types de pannes : Les pannes des applications et les pannes du cluster.

==== Pannes des applications

La gestion des pannes des applications est gérée par Kubernetes.

Pour y arriver, Kubernetes se base sur https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/[les lignes de vie des applications].

Si la ligne de vie d'une application ne répond pas, Kubernetes se charge de redémarrer l'application.

Chaque application déployée doit donc définir ses lignes de vie.

....
Scenario: APP_HEALTHCHECK
En tant que Développeur d'une application,
Quand le service ligne de vie d'un des services de mon application ne répond plus,
Alors Kubernetes redémarre l'application
....

==== Pannes du Cluster

La gestion des pannes du cluster est gérée par une installation multi-maîtres :

* Pour être tolérant à une panne, il faut 3 maîtres
* Pour être tolérant à deux pannes, il faut 5 maîtres